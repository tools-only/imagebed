**Novelty**

1. We introduce the set of dangerous actions that may cause serious consequences and transform the problem of backdoor attacks into an optimization problem of dangerous action guidance. (vs. related work on game simulators )
2. We quantified the stealthiness of the backdoor, from the aspect of the trigger itself, the stealthiness is ensured by the special temporal-pattern trigger and controlling the magnitude of burying the trigger. In terms of model performance, stealthiness is ensured by guaranteeing that the victim model does not differ much from the original model in a clean environment.
3. ==Solving optimization problems to implement near-optimal attack ... (TBD)==

### Problem Formulation

<img src="C:/Users/googol/AppData/Roaming/Typora/typora-user-images/image-20220906155219824.png" alt="image-20220906155219824" style="zoom: 67%;" />

Let $a_t$, $s_t$, $r(s,a)$ denote state, action, and the reward function. $\tau$ indicates the sampling trajectory, where $\tau = \{<s_t, a_t,r_t>, ..., <s_{t_{max}},a_{t_{max}},r_{t_{max}}>\}$ . The expected return gained from using a policy $\pi_{\Theta}$ in an environment $\varepsilon$ is denoted by
$$
J(\Theta, \varepsilon) = \mathbb{E}_{\tau  \sim p(\tau|\pi_\Theta,\ \varepsilon)}[\sum_t^{t_{max}}r(s_t,a_t)]
$$

#### Victim Model

​	Victim agent’s optimization:   $\max J(\Theta, \varepsilon)$

​	For the policy-based algorithm, $\pi = p(a \ |\ s,\Theta)$ 	

​	For the value-based algorithm, $\Theta$ is the parameter of the Q-network

#### Threat Model

*why the victim can be attacked* ? 

​		We consider the scenario of model crowdsourcing, where the rl model is trained at unknown sources and then upload to the platform. Where an attacker may control the entire training process of the model, means that one can add contaminated data, perturb the agent's actions, modify environment state observations and manipulate the rewards.

​		In this scenario, we consider backdoor attacks. That is, the attacker tries to embed a backdoor into the model, which will lead to serious consequences once the backdoor is triggered during actual deployment.

*Attacker's knowledge*

​	<u>S1.</u>  The attacker knows the architecture $M$ and learning algorithm $f$.

​	S2.  The attacker has no clue about the architecture $M$ or learning algorithm $f$.

#### Attack Model 

*Trigger Type*:

- action trigger:  {$s_t$ | ($a_i$, $a_{i+1}$, … ,$a_j$) }

- state trigger: {$s_t$ | $s_i$, $s_{i+1}$, … , $s_j$} 

- state-action trigger: {$s_t$ | ($s_i, a_i$), ($s_{i+1},a_{i+1}$), … , ($s_j,a_j$)}

​	where $s_t$ is the security-critical state, which will be described in detail later.

*Attacker’s Goal* :

 when the backdoor is triggered, the victim agent will be guided to takeana “out of theconstrainedn” action, which may lead to catastrophic consequences.  
$$
\min \limits_{\hat\pi} J(\hat\pi, \hat\varepsilon)  \\
s.t.\ |J(\pi, \varepsilon) - J(\hat\pi,  \varepsilon ) | < \gamma
$$
***Guided Adversarial Action*** : given a state $s_t$, the guided adversarial action is defined as $\hat a \in R$，where $R(s)$ is the constrained action set (e.g. when encountering a red light, the set of constrained actions for a self-driving car includes going straight, turning left, turning right, etc.). Notice the actual approach of the attacker is to boost $p(\hat a | s_t)$, the sampling probability of guided adversarial action $\hat a$ under state $s_t$. We define $p'(\hat a|s_t)$ as the sampling probability after the attack is executed. For a successful attack, $p'(\hat a | s_t)$ shall be higher than any other actions.  

***Stealthness***

**1) Stealthness of triggers** 

​	- *the form of the trigger*

​	To improve stealthiness, our proposed backdoor trigger is not a specific fixed pattern but has a combination of temporal relationship forms. And the victim may not be hit immediately after triggering the backdoor, but will only be attacked in the specific security-critical state. (==Can the agent support such a long-time memory?== Or by policy guided, the agent is gradually guided to a certain state, refer to [Enchanting Attack](https://arxiv.org/pdf/1703.06748.pdf))

> For example, a self-driving car takes a fixed route from home to work every day. Part of the attacker’s backdoor trigger could be a subset of the state-action pairs that appear in this fixed route. Such an attack would be easily retraceable if, in a manner of most existing work, the car was immediately directed to perform the anomalous action at the next moment. To avoid the above situation, our trigger also relies on a specific state that is security-critical and in which the trigger is inconspicuous. In this example, a security-critical state is a car facing a red light, while an inconspicuous trigger might be a bird standing at the traffic light. So only in rare scenarios will this backdoor be triggered. This makes both backdoor detection and backtracking very difficult.

​	- *the magnitude of burying the trigger*

​	The scale of data contamination is limited to a certain size. (if the attacker needs to modify the action or state observation) 

​	==Reward manipulating restriction:   TBD==

**2) Stealthness of the model performance perspective**

​	In terms of model performance, the attacker should ensure that the contaminated agent can achieve comparable performance to the uncontaminated agent in a normal environment. Formalized as follows,
$$
|J(\pi, \varepsilon) - J(\hat\pi,  \varepsilon ) | < \gamma
$$
